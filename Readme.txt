In recent years, Visual Question Answering (VQA) has emerged as a promising approach in medical diagnostics, enabling automated systems to respond to natural language queries based on medical images. This study proposes an intelligent deep learning-based VQA framework specifically designed for radiological images, utilizing the VQA-RAD dataset. The framework incorporates a pre-trained VGG16 convolutional neural network for extracting spatial features from CT and MRI images, along with a Bidirectional Long Short-Term Memory (BiLSTM) network to process and understand clinical questions. Word embeddings generated from the Google News Word2Vec model are employed to convert textual inputs into semantically meaningful vector representations. The visual and textual features are fused and processed through fully connected layers to predict the most probable answer. The model is trained and evaluated on a curated radiology dataset and achieves a test accuracy of 80%. To support practical deployment, an interactive interface is integrated, enabling users to upload radiological scans and pose clinical questions in natural language, with the system returning confidence-scored responses in real time. This framework demonstrates the potential of deep learning in enhancing clinical decision-making through explainable, image-grounded question answering in radiological contexts.
